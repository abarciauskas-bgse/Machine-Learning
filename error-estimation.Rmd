---
title: Error Estimation
author: Aimee Barciauskas
output: pdf_document
date: 29 January 2016
---

**Overfitting Error**

$$R(g_{n}) - R_{n}(g_{n}) \leq sup_{g \in \mathcal{C}} |R_{n}(g) - R(g)|$$

**Excess Risk**

$$R(g_{n}) - inf_{g \in \mathcal{C}}R(g) \leq 2sup_{g \in \mathcal{C}} |R_{n}(g) - R(g)|$$

*Remark:* All of these hold for general loss functions.

**Radebacher Average:**

$$\mathbb{E}_{\sigma}\{ |\frac{1}{n} \sum_{i=1}{n}\sigma_{i}\mathbb{I}_{X_{i\inA}| \}$$

We use the bounded differences inequality to bound

$$\leq 2 \mathbb{E}\{\mathbb{E}_{\sigma}\{ |\frac{1}{n} \sum_{i=1}{n}\sigma_{i}\mathbb{I}_{X_{i\inA}| \}\} + \sqrt{\frac{log(\frac{2}{\delta})}{2n}}$$

But since we know from the concentration inequality that the difference between the realization and it's expected value.

$$\leq 2 \mathbb{E}_{\sigma}\{ |\frac{1}{n} \sum_{i=1}{n}\sigma_{i}\mathbb{I}_{X_{i\inA}| \} + 3\sqrt{\frac{log(\frac{2}{\delta})}{2n}} \text{ with probability $\geq 1-2\delta$}$$

Probability on the RHS is purely empirical, iow, computable from the data

This is distribution free and non-asymptotic

## VC dimension

Understand the behavior of the Radebacher average with the VC dimension

**Bounding the Rademacher Average with the VC dimension**

$$\rho_{n}(\mathcal{A}) = Rademacher average$$
$$ \leq sqrt(\frac{2log(S(X^{n},\mathcal{A}))}{n})$$

We cand bound this by the worst-case shatter coefficient

$$ \leq sqrt(\frac{2log(S_\mathcal{A}(n))}{n})$$

Worst-case shatter coefficients:

* If $\mathcal{A} =$ class of half-intervals $(-\infty,a]$ then $S_{\mathcal{A}}(n) = n+1$
* If $\mathcal{A} =$ class of all intervals then $S_{\mathcal{A}}(n) = \frac{n(n+1)}{2}$

In general, the largest integer $k$ such that $S_{\mathcal{A}}(n) = 2^{k}$ is the **VC dimension** of $\mathcal{A}$, denoted by $V_{\mathcal{A}} = V$

If $\mathcal{A}$ is such that $S_{\mathcal{A}}(n) = 2^{k}$ for all $n$, we say the VC dimension is $\infty$

**Sauer's Lemma**

Bounds the shatter coefficient in terms of the VC dimension.

For all $n$,

$$S_{\mathcal{A}}(n) \leq \sum_{i=0}^{V} {n choose i} \leq (n+1)^{V}$$

Upper bound is polynomial, which means we can bound the rademacher average futher by:

$$\leq sqrt(\frac{2V_{\mathcal{A}} log(n+1)}{n})$$

Return to bounding the empirical risk / overfitting error:

$$\leq 2sqrt(\frac{2V_{\mathcal{A}} log(n+1)}{n})$$

Sometimes this ^^ is called Vapnick-Chervonakis inequality (the *VC inequality*)

No matter what, the overfitting error is bounded by a combinatorial quantity divided by n

## Examples of the shatter coeffificient

1. Half-spaces

$$\mathcal{A}=\{ (-\infty,a]: a \in \mathbb{R} \}$$
$$S_\mathcal{A}(n) = n+1$$
$$V_{\mathcal{A}} = 1; S_\mathcal{A}(n) \leq {n choose 0} + {n choose 1} = n + 1$$

2. Intervals

$$\mathcal{A}=\{ [a,b]: a \leq b \}$$
$$S_\mathcal{A}(n) = \frac{n(n+1)}{2} + 1$$
$$S_\mathcal{A}(n) \leq {n choose 0} + {n choose 1} + {n choose 2}= 1 + n + \frac{n(n-1)}{2} = \frac{n(n+1)}{2} + 1$$

3. Rectangles in $\mathbb{R}^{2}$

$$\mathcal{A} = \text{ class of all rectangles in } \mathbb{R}^{2}$$
$$ = \{ [a,b] \times [c,d]: a \leq b, c \leq d\}$$

Not easily to calculate

*Claim:* Five (5) points cannot be shattered. Take the right-most and left-most, north-most, south-most, there is no rectangle does not contain the left-out (inner) point.

The VC dimension is 4!

4. All rectangles in $\mathbb{R}^{d}$

$$ \mathcal{A} = \{ [a,b] \times [a_{2},b_{2}] \times ... \}$$
$$V_{d} = 2d$$

**Pattern** Same VC dimension as dimension to the class. (But this is not always the case, so don't get too excited)

5. $\mathcal{A}$ is a class of all convex polygons in $\mathbb{R}^2$

Take class of all convex polygons in $\mathbb{R}^{2}$

Like a hexagon, or a k-gon.

The VC dimension is infinite because if all are placed on a circle then any subset can be intersected by a convex polygon.

## Upper-bounding VC dimension

(Next example of calculating the VC-dimension)

In the important case of linear classifiers (split the space along a hyperplane)

$$\{x: x^{T}w + b \geq 0\} \text{ The set of all linear half-spaces in }\mathbb{R}^{2} $$

**Theorem** Let $\phi_{1},...,\phi_{n} : \mathcal{X} \to \mathbb{R}$ be fixed functions. (e.g. "feature mappings")

We have our feature space, for each $x$ we calculate $\phi(x)$ to map into $\mathbb{R}^{d}$

Consider the class of all sets of the form:

$$\mathcal{A} = \{ \{ x \in \mathcal{X} : \sum_{i=1}{r} a_{i} \phi_{i}(x) \geq 0 \} : a_{1},...,a_{r} \in \mathbb{R} \}$$

Then the VC dimension of $\mathcal{A}$ is:

$$V_{\mathcal{A}} \leq \r$$

Useful when calculating the VC dimension of something like the VC dimension of the sets of half-circles. Which is a linear combination of 5 feature maps.

$$\phi_{1}(x,y) = x^{2}$$
$$\phi_{2}(x,y) = x$$
$$\phi_{3}(x,y) = y^{2}$$
$$\phi_{4}(x,y) = y$$
$$\phi_{5}(x,y) = 1$$
(weighted...)






























