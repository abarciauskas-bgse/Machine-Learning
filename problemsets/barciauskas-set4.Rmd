---
title: Machine Learning Problemset 4
author: Aimee Barciauskas
date: 12 March 2016
output: pdf_document
---


# Problem 17

**Let $(x_{1}, y1), ..., (x_{n}, y_{n})$ be data in $\mathbb{R}^{d} \times \{-1,1\}$**. Suppose the data are *linearly seperable* ...

Since the data are linearly seperable, we can define a separating hyperplane:

$$\Big\{ x : f(x) = w^{T}x = 0 \Big\} \text{ where } \|w\| = 1$$

The seperating hyperplane returns a signed distance to the plane for each $x_{i}$, and classification is done according to the sign:

$$f(x) : sgn \big[w^{T}x\big]$$

The margin of this classifier is as stated in the problem:

$$\gamma(w) = \min\limits_{i} \frac{y_{i}w^{T}x_{i}}{\|w\|}$$

An optimal $f(x)$ is one which maximizes the margin $\gamma$, i.e.:

$$\gamma^{*} = \max \gamma_{w, \|w\| = 1} \text{  s.t.  } y_{i}w^{T}x_{i} \geq \gamma$$

The constratint on the norm of $w$, $\|w\| = 1$ can be removed when:

$$ \frac{y_{i}w^{T}x_{i}}{\|w\|} \geq \gamma$$

and by setting $\|w\| = \frac{1}{\gamma}$ the maximazation optimization problem becomes a convex minimization problem of the form:

$$\min\limits_{w}\|w\| \text{  s.t.  } y_{i}w^{T}x_{i} \geq 1$$

*Solution Part 2*

Minimizing $\|w\|$ is equivalent to minimizing $\frac{1}{2}\|w\|^{2}$, which is helpful because this function is continuously differentiable and thus we can find the optimal $w^{*}$ using the Lagrangian approximation:

$$\mathcal{L} = \frac{1}{2}\|w\|^{2} - \sum\limits_{i=1}^{n} \lambda_{i} y_{i}w^{T}x_{i} - 1$$

Taking the derivative and setting to 0:

$$\Delta_{w} \mathcal{L} = \frac{1}{2}2w - \sum\limits_{i=1}^{n} \lambda_{i}y_{i}x_{i} = 0$$

Solving for $w^{*}$:

$$w^{*} = \sum\limits_{i=1}^{n} \lambda_{i}y_{i}x_{i}$$

This is a linear combination of $x_{i}$'s which are on the margin so in the same vector space as thos $x_{i}$.

# Problem 18

*Solution Part 1*

$$y_{n} \in \big\{-1, 1\big\}$$
$$x_{n} \in \big\{0, 1\big\}$$

The classifier is defined by the weight vector:

$$w = (w_{0},...,w_{d})$$

Being linearly seperable means, by definition, there exists such a $w^{T}x_{i} > 0$ whenever $y_{i} = 1$ and $w^{T}x_{i} \leq 0$ whenever $y_{i} = -1$

By the statement of the problem, $w^{T}x_{i} > 0$ whenever at least one $x_{i,j} = 1$. The $w$ satisfying this is the $w$ where $w_{0}$ is $\big(-1,0\big)$ and all $w_{1} = ... = w_{d} = 1$:

$$\Big\{w : -1 < w_{0} < 0 \text{ and } w_{1} = ... = w_{d} = 1\Big\}$$

So $w^{T}x$ will be greater than 0 where at least one $x_{i}$ is 1 and $w_{0}$ otherwise. The $x_{i}$'s nearest the $w$ vector are the $x_{i}$'s where all elements are zero and their counterparts: those with only one dimension being equal non-zero, i.e. the $x_{i}$'s satisifyiing:

$$\sum\limits_{m=1}^{d}x_{m,i} = 1, \text{ where $d$ is the dimension of $x$}$$

These two types of points define the location of the hyperplane. The margin maximizing the distance between between such $x_{i}$'s is where the margin equivalent for both these points:

$$\frac{y_{j}w^{T}x_{j}}{\|w\|} = \frac{y_{i}w^{T}x_{i}}{\|w\|}$$

Where $y_{j} = -1$, $y_{i} = 1$ and $\sum\limits_{m=1}^{d}x_{m,j} = 0$, $\sum\limits_{m=1}^{d}x_{m,i} = 1$

To solve for the optimal $w*$, where we know all $w_{1} = ... = w_{d} = 1$ so we are just solving for $w_{0}$:

$$w_{0} = - \frac{1}{2}$$

Plugging this back into the equation for $\gamma$:

$$\gamma^{*}(w) = \frac{1}{2\sqrt{\frac{1}{4} + d}}$$


*Solution Part 2*

The problem defines $y_{i}=1$ if and only if the sum of the components of $x_{i}$ is at least $\frac{d}{2}$:

We prove that always it is possible to define $w$ such that:

$$w^{T}x > 0 \text{ when } \sum_{i} x_{i} > \frac{d}{2}$$

We define the weight vector as $w^{T}x = w_{0}^{*} + kw_{1,...,d}^{*}$ where $k$ is the number of non-zero elements of $x$ and $w^{*}$ is the optimal weight vector, and the proof is to and $w_{0}^*$ such that:

$$w^{*^{T}}x > 0 \text{ when } k \geq \frac{d+1}{2} \text{ and}$$

$$w^{*^{T}}x < 0 \text{ when } k < \frac{d-1}{2}$$

Say $-w_{0}^{*} = kw_{1,...,d}^{*}$ and we can set $w_{1,...,d}^{*} = 1$ and there are two scenarios:

1. $w_{0}^{*} + \frac{d+1}{2} > 0 \text{ when } k \geq \frac{d+1}{2}$
2. $w_{0}^{*} + \frac{d-1}{2} < 0 \text{ when } k < \frac{d-1}{2}$

So the possible values of $w_{0}$ are:

$$ \frac{d-1}{2} < -w_{0}^{*} < \frac{d+1}{2}$$

$x_{i}$'s nearest the classifying hyperplane, are those where:

$$\sum_{m=1}^{d}x_{ij} = \frac{d-1}{2} \text{ or }$$
$$\sum_{m=1}^{d}x_{ij} = \frac{d+1}{2}$$

I.e.:

$$y_{i} = 1, \sum_{i}x_{i} = \frac{d+1}{2}$$
$$y_{j} = -1, \sum_{j}x_{j} = \frac{d-1}{2}$$

$\gamma(w)$ is the same for all $x_{i}$ so:

$$y_{i}w^{T}x_{i} = y_{j}w^{T}x_{j}$$

Substituting $y_{i}, y_{j}$ with $-1,1$:

$$w^{T}x_{i} = -w^{T}x_{j}$$

$$w_{0} + \sum_{m=1}^{d}w_{m}x_{i,m} = - w_{0} - \sum_{m=1}^{d}w_{m}x_{j,m}$$

$$w_{0} + \frac{d+1}{2} = - w_{0} - \frac{d-1}{2}$$

$$w_{0} = - \frac{d}{2}$$

Plugging this into the equation for $\gamma$:

$$\gamma^{*}(w) = \frac{1}{2 \sqrt{d} }$$


# Problem 19

*Solution Part 1*

$$K(x,y) = \big \langle \phi(x),\phi(y) \big \rangle$$

where:

$$\phi(x)_{n} = \frac{1}{\sqrt{n!}}x^{n}e^{ \frac{-x^{2}}{2}}$$

$$\big \langle \phi(x),\phi(y) \big \rangle = \sum_{n=0}^{\infty} \frac{1}{\sqrt{n!}}x^{n}e^{\frac{-x^{2}}{2}}\frac{1}{\sqrt{n!}}y^{n}e^{ \frac{-y^{2}}{2}}$$

Simplifying and using that $\sum_{n=0}^{\infty} \frac{x^{n}}{n!} = e^{x}$:

$$= e^{xy - \frac{1}{2}(x^{2} + y^{2})}$$

Multiply the exponent by $\frac{2}{2}$:

$$= e^{- \frac{\|x-y\|^{2}}{2}}$$

This is the gaussian kernel.

*Solution Part 2*

Generalize to $\mathbb{R}^{d}$

$$=\sum_{i=1}^{\infty} \frac{1}{n!}\Big(\sqrt{(x^{T}x)}\sqrt{y^{T}y}\Big)^{n}exp\Big(- \frac{1}{2}(x^{T}x)- \frac{1}{2}(y^{T}y)\Big)$$

$$= exp(\sqrt{(x^{T}xy^{T}y)} - \frac{1}{2}x^{T}{x}- \frac{1}{2}y^{T}y)$$

$$= exp(x^{T}y - \frac{1}{2}x^{T}{x} - \frac{1}{2}y^{T}y)$$

$$= exp( \frac{2x^{T}y - x^{T}{x} - y^{T}y}{2})$$

$$= exp(- \frac{\|x-y\|^{2}}{2})$$


# Problem 20

*Solution Part 1*

$$K(x,y) = \big \langle \phi(x),\phi(y) \big \rangle$$
$$= \bigg \langle \binom{\phi_{1}(x)}{\phi_{2}(x)}, \binom{\phi_{1}(y)}{\phi_{2}(y)} \bigg \rangle$$
$$= \big \langle \phi_{1}(x), \phi_{1}(y) \big \rangle + \big \langle \phi_{2}(x), \phi_{2}(y) \big \rangle$$
$$= K_{1}(x,y) + K_{2}(x,y)$$

*Solution Part 2*

$$K(x,y) = \phi(x)^{T}\phi(y)$$
$$= \phi_{1}(x)^{T}\phi_{1}(y)\phi_{2}(x)^{T}\phi_{2}(y)$$
$$= \big(\phi_{1}(x)\phi_{2}(x)\big)^{T}\big(\phi_{1}(y)\phi_{2}(y)\big)$$
$$= K_{1}(x,y)K_{2}(x,y)$$


# Problem 21

*Solution Part 1*

There are $2^{m}$ possible substrings $s$, so we define the dimension to be $\mu = 2^{m}$

The feature mapping $\phi(x)$ maps a string, $x$ to the $1 \times 2^{m}$ feature space defined by the indicator function:

$$\phi(x)_{\mu} = \sum_{i=1}^{\mu} \mathbb{I}_{s_{i} \text{ substring of } x}$$

The kernel function for 2 strings is:


$$\big \langle \phi(x)_{\mu},\phi_{\mu}(y) \big \rangle = K(x,y)$$

$$= \sum_{i=1}^{2^{m}} \mathbb{I}_{s_{i} \in x} \mathbb{I}_{s_{i} \in y}$$
$$= \sum_{i=1}^{2^{m}} \mathbb{I}_{s_{i} \in x,y}$$
$$= \sum_{s_{i} \in {0,1}^{m}} \mathbb{I}_{s_{i} \in x,y}$$

*Solution Part 2* 

Let $J$ be the set of all possible $s$, the magnitude of $J$ is the dimension of the $\mathcal{H}$ the hilbert space of this kernel function, that is the dimension of the Hilbert space of the string kernel is $2^{m}$, i.e.:

$$\phi(x) = \mathbb{R}^{n} \to \mathbb{R}^{\mu}$$


