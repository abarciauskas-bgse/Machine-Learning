---
title: Machine Learning Problemset 2
author: Aimee Barciauskas
date: 12 February 2016
output: pdf_document
---

## 7. Let the joint distribution of $(X, Y)$ be such that $X$ is uniform on the interval [0, 1], and for all $x \in [0,1], \eta(x) = x$. Determine the prior probabilities $\mathbb{P}\{Y = 0\},\mathbb{P}\{Y = 1\}$ and the class-conditional densities $f(x|Y = 0)$ and $f(x|Y = 1)$. Calculate $R^{*}, R_{1-NN}, R_{3-NN}$ (i.e., the Bayes risk and the asymptotic risk of the 1-, and 3-nearest neighbor rules).

### $R^{*}$

$$R^{*} = \int_{0}^{\frac{1}{2}}\eta(x)dx + \int_{\frac{1}{2}}^{1}(1-\eta(x))dx$$

$$=\frac{x^{2}}{2}\Big|_{0}^{\frac{1}{2}} + [x - \frac{x^{2}}{2}] \Big|_{\frac{1}{2}}^{1} = \frac{1}{4}$$

### $R_{1-NN}$

*From in-class calculation:*

$$R_{1-NN} = \eta(x)(1-\eta(x)) + (1-\eta(x)\eta(x)$$
$$= 2\eta(x)(1-\eta(x))$$
$$= \int [2\eta(x)(1-\eta(x))] dx$$

Substitute $x = \eta(x)$:

$$= 2\int x(1-x) dx$$
$$= 2[\frac{1}{2}x^{2} - \frac{1}{3}x^{3}] \Big|_0^1$$
$$R_{1-NN} = \frac{1}{3}$$

### $R_{3-NN}$

From in-class calculation we know:

$$R_{3-NN} = \mathbb{E}[\eta(x)(1-\eta(x))] + 4 \mathbb{E}[\eta(x)^{2}(1-\eta(x))^{2}]$$

Following similar steps from $R_{1-NN}$ we get:

$$= \frac{1}{2}x^{2} + \frac{3}{3}x^{3} - \frac{8}{4}x^{4} + \frac{4}{5}x^{5} \Big|_0^1$$
$$R_{3-NN} = \frac{3}{10}$$

## 8. Let $X_{1},...,X_{n}$ be independent random variables taking values in $[0,1]$. Denote $m = \mathbb{E}_{i=1}{n} X_{i}$. Prove that for any t: $$\mathbb{P}\Bigg\{ \sum_{i=1}^{n} X_{i} \geq t \Bigg\} \leq \bigg(\frac{m}{t}\bigg)^{t} e^{t-m}$$ *Hint:* Use Chernoffâ€™s bounding technique. Use the fact that by convexity of $e^{\lambda x}, e^{\lambda x} \leq xe^{\lambda} + (1-x)$

Start with the equality:

$$\mathbb{P}\{X \geq t\} = \mathbb{P}\{e^{\lambda X} \geq e^{\lambda t}\}$$

Using Chrenoff, we know this probability is less than or equal to:

$$\mathbb{P}\{e^{\lambda X} \geq e^{\lambda t}\} \leq \frac{\mathbb{E}[e^{\lambda X}]}{e^{\lambda t}}$$

We can set $\lambda = log(\frac{t}{m})$ because $t \geq m$ this will always be positive.

$$= \frac{\mathbb{E}[e^{\lambda X}]}{(\frac{t}{m})^{t}}$$

$$= (\frac{m}{t})^{t} \mathbb{E}[e^{\lambda X}]$$

Using the hint $e^{\lambda x}, e^{\lambda x} \leq xe^{\lambda} + (1-x)$:

$$\leq (\frac{m}{t})^{t} \mathbb{E}[Xe^{\lambda} + 1 - X]$$

$$= (\frac{m}{t})^{t} [m e^{\lambda} + 1 - m]$$

Substitute lambda:

$$= (\frac{m}{t})^{t} [t - m + 1]$$

The second term is always less than $e^{t-m}$:

$$\leq (\frac{m}{t})^{t} e^{t-m}$$


## 9. Let $R_{k-NN}$ denote the asymptotic risk of the k-nearest neighbor classifier, where k is an odd positive integer. Use the expression of $R_{k-NN}$ found in class to show that:

$$R_{k-NN} - R^{*} \leq \sup_{p \in [0,\frac{1}{2}]} (1-2p) \mathbb{P}\{Bin(k,p) > k/2\}$$

*Proof:*

[ADD ME]

$$\leq \sup_{p \in [0,1/2]} (1-2p) \mathbb{P}\{Bin(k, min(\eta, 1-\eta) > \frac{k}{2}\}$$

Now we use Hoeffding's inequality to proof:

$$R_{k-NN} - R^{*} \leq \frac{1}{\sqrt{ke}}$$

For simplicity, we declare $B = Bin(k,p)$, and $p = min(\eta, 1-\eta)$ and we can reduce

$$\mathbb{P}\{Bin(k, min(\eta, 1-\eta) > \frac{k}{2}\} = \mathbb{P}\{B > \frac{k}{2} \}$$
$$\mathbb{P}\{B > \frac{k}{2} \} = \mathbb{P}\{ B - kp > k(\frac{1}{2} - p)\}$$

Theorem 8.1, where $S_{n}$ is a sum of random variables:

$$\mathbb{P}\{S_{n} - \mathbb{E}S_{n} \geq \epsilon\} \leq exp\{-2\epsilon^{2}/\sum(b_{i}-a_{i})^{2}\}$$

Since $B$ is a sum of random variables and $kp$ is it's expected value, the above expression can be expressed:

$$\mathbb{P}\{ B - kp > k(\frac{1}{2} - p)\} \geq exp\{-2k(1-p)/\sum (b_{i} - a_{i})^{2}\}$$

The sum in the exponent is always a sum of the difference between a binary variable, so it is a sum of k 1's.

$$\mathbb{P}\{B > \frac{k}{2} \} \geq exp\{-2k(1/2 -p)^{2}\}$$

Replacing $p$ with $u$ for simplicity ($u = 1 - 2p$) the entire expression can be written as:

$$\sup_{p \in [0,\frac{1}{2}]} (1-2p) \mathbb{P}\{Bin(k,p) > k/2\} \leq \sup_{0 \leq u \leq 1} ue^{-ku^{2}/2}$$

which is equivalent to

$$ = \frac{1}{\sqrt{ke}}$$

## 10. (RADEMACHER AVERAGES) Let $\mathcal{A}$ be a bounded subset of $R_{n}$. Define the Rademacher Average: $$ R_{n}(A) = \mathbb{E} \sup_{a \in A} \frac{1}{n} \left| \sum_{i=1}^{n} \sigma_{i}a_{i} \right| $$. where $\sigma_{1},...,\sigma_{n}$ are independent random variables with $\mathbb{P}\{\sigma_{i}=1\}=\mathbb{P}\{\sigma_{i}=-1\}=\frac{1}{2}$. Prove the following "structural" results:

### $$R_{n}(A \cup B) \leq R_{n}(A) + R_{n}(B)$$

*Proof:*

$$R_{n}(A \cup B) = \mathbb{E} \sup_{v \in A \cup B} \frac{1}{n} \left| \sigma_{i}v_{i} \right| $$

$$ \leq \mathbb{E} \sup_{a \in A} \frac{1}{n} \left| \sum_{i=1}^{n} \sigma_{i}a_{i} \right| + \mathbb{E} \sup_{b \in B} \frac{1}{n} \left| \sum_{i=1}^{n} \sigma_{i}b_{i} \right| $$

### $$R_{n}(c \bullet A) = |c| R_{n}A$$

*Proof*

$$R_{n} (c\bullet A) = \mathbb{E} \sup_{a \in c \bullet A} \frac{1}{n} \left| \sum_{i=1}^{n} \sigma_{i} c a_{i} \right|$$
$$= \left|a\right| \mathbb{E} \sup_{a \in \bullet A} \frac{1}{n} \left| \sum_{i=1}^{n} \sigma_{i}a_{i} \right|$$

### $$R_{n}(A \oplus B) = R_{n}(A) + R_{n}(B)$$

*Proof*

$$R_{n}(A \oplus B) = \mathbb{E} \sup_{v \in A + B} \left| \frac{1}{n}\sum_{i=1}^{n}\sigma_{i}v_{i} \right|$$

$$\mathbb{E} \sup_{a \in A, b \in B} \left| \frac{1}{n} \sum_{i=1}^{n} \sigma_{i}(a_{i} + b_{i}) \right|$$

$$\leq \mathbb{E} \sup_{a \in A} \frac{1}{n} \left| \sigma_{i}a_{i} \right| + \mathbb{E} \sup_{b \in B} \frac{1}{n} \left| \sigma_{i}b_{i} \right|$$

$$= R_{n}(A) + R_{n}(B)$$

### If $absconv(A) = \{\sum_{j=1}^{N}c_{j}a^{j} : N \in \mathbb{N}, \sum_{j=1}^{N} \left| c_{j}\right| \leq 1, a^{j} \in A \} \text{ is the absolute convex hull of A, then:}$

The absolute convex hull of $A$ is the union of all sets:

$$c_{1}A_{1} + ... + c_{N}A_{N} = \{c_{1}a_{1} + ... + c_{N}a_{N} : a_{1}, ..., a_{N} \in A\}$$

then, the rademacher average of a given set:

$$R_{n}(c_{1}A_{1} + ... + c_{N}A_{N}) = \sum_{j=1}^{n} \left| c_{j}\right| R_{n}(A) \leq R_{n}(A)$$

So for all $N$ choices of $c_{j}$, the Rademacher average of absolute convex hull of the set $A$ is less than or equal to the Rademacher average of $A$, so the Rademacher average of A is equal to the Rademacher average of A.

## 11. A half plane is a set of the form $H_{a,b,c} = \{(x,y) \in \mathbb{R}^{2} : ax+by \geq c\}$ for some real numbers $a,b,c$. Determine the n-th shatter coefficient of the classes:

$$\mathcal{A}_{0} = \{H_{a,b,0} : a,b \in \mathbb{R}\} \text{ and } \mathcal{A}_{0} = \{H_{a,b,c} : a,b,c \in \mathbb{R}\}$$

In the first case, $\mathcal{A}_{0} = \{H_{a,b,0} : a,b \in \mathbb{R}\}$, a half plane must pass through the origin so it can only rotate around the origin. So $n$ points can be subset into $n+1$ different ways. So the n-th shatter coefficient is $n+1$.

*Corollary 13.1* defines the shatter coefficient for the class of all half-spaces. Adapting the equation for the class of half-spaces in $\mathbb{R}^{2}$ gives:

$$s(\mathcal{A},n) = 2\sum_{i=0}^{2} {n-1 \choose i}$$

Expanding the sum we find the n-th shatter coefficient for the class of half-spaces in $\mathbb{R}^{2}$:

$$s(\mathcal{A},n) = 2n + (n-1)(n-2)$$





