---
title: Machine Learning Problemset 3
author: Aimee Barciauskas
date: 26 February 2016
output: pdf_document
---

## **Problem 12**

**Consider the class $\mathcal{A}$ of all sets of the form:**

$$A_{\alpha} = \{ x \in \mathbb{R} : sin(\alpha x) > 0 \}$$

**where $\alpha > 0$. What is the VC dimension of $\mathcal{A}$? (Note that $\mathcal{A}$ has one free parameter.)**

The VC dimension of $\mathcal{A}$ is infinite.

For example, if $x_{i} = 2^{-i}, i = 1, ..., m$ are assigned arbitrary labels $(y_{1},...,y_{m}) \in \{-1,1\}^{m}$, $\alpha$ may be chosen such that any label set is correctly classified:

$$\alpha = \pi(1 + \sum_{i=1}^{m} 2^{i}\frac{1-y_{i}}{2})$$

## **Problem 13** 

**Let $\mathcal{A}_{1}, ... , \mathcal{A}_{k}$ be classes of sets, all of the with VC dimension at most $V$. Show that the VC-dimension of $\cup_{i=1}^{k} \mathcal{A_{i}}$ is at most $4Vlog_{2}(2V) + 4k$. You may use the fact that for $a \geq 1$ and $b > 0$, if $x \geq 4a log(2a) + 2b$ then $x \geq a log x + b$.**

**Can you bound the VC dimension of the class of all sets of the form:**

$$A_{1} \cup ... \cup A_{k} \text{ with } A_{1} \in \mathcal{A}_{1}, ..., A_{k} \in \mathcal{A}_{k}$$

*Part 1 Solution:*

$$s_{\mathcal{A}} = 2^{u} \text{ such that $\alpha$ is the VC dimension of $\mathcal{A}$}$$

Sauer's Lemma:

$$s_{\mathcal{A}} \leq s_{\mathcal{A}_{1}} + s_{\mathcal{A}_{2}} + ... + s_{\mathcal{A}_{k}} \leq k(u+1)^{V} $$

If there exists a $u$ which satisfies:

$$s_{\mathcal{A}} = 2^{u}$$

$$2^{u} \leq k(u+1)^{V}$$

$$u \leq log(k) + Vlog(u+1)$$

$$ = log(k) + Vlog(u+1)$$

$$log(k) \leq 2k$$

$$= 2k + Vlog(u+1)$$

Using $log(u+1) \approx log(u)$

$$u \leq 2k + Vlog(u)$$

Using the hints from the problem where $a = V$, $x = u$ and $b = 2k$ this can be re-written as:

$$u \leq 4Vlog(2V) + 4k$$

*Part 2 Solution:*

For the union of sets of $A_{1} \cup ... \cup A_{k}$:

$$s_{\mathcal{A}}(n) \leq s_{\mathcal{A}_{1}}(n) \times s_{\mathcal{A}_{1}}(n) \times ... \times s_{\mathcal{A}_{k}} \leq (n+1)^{Vk}$$

$$s_{\mathcal{A}}(u) = 2^{u} \leq (u+1)^{Vk}$$

$$u \leq Vklog(u+1)$$

Let $a = Vk$, $b=\epsilon$, $x=u+1$, where $\epsilon$ is some small number:

$$u \leq 4Vklog(2Vk)$$

## **Problem 14**

$$\lVert w_{t} - w_{*} \rVert^{2} \leq \lVert w_{t-1} - w_{*} \rVert^{2} - 1$$

We have $w_{t} = w_{t-1} + \frac{Y_{t}X_{t}}{\lVert X_{t}\lVert }$, so the first term can be re-written as:

$$\lVert w_{t-1} - w_{*} + \frac{Y_{t}X_{t}}{\lVert X_{t}\lVert} \lVert ^{2} = \lVert w_{t-1} - w_{*} \rVert^{2} + \bigg(\frac{Y_{t}X_{t}}{\lVert X_{t}\lVert}\bigg)^{2} + 2(w_{t-1} - w_{*}) \frac{Y_{t}X_{t}}{\lVert X_{t}\lVert}$$

$$= \lVert w_{t-1} - w_{*} \rVert^{2} + \bigg(\frac{Y_{t}X_{t}}{\lVert X_{t}\lVert}\bigg)^{2} + 2w_{t-1}\frac{Y_{t}X_{t}}{\lVert X_{t}\lVert} - 2w_{*}\frac{Y_{t}X_{t}}{\lVert X_{t}\lVert}$$

The first term above equals the first term in the RHS of our initial inequality to be proved.

The second term is 1, so we subtract it from the RHS and get an equality of the first two terms of the expanded LHS and RHS.

*The last two terms formulate the inquality:* We know the second to last term $2w_{t-1}\frac{Y_{t}X_{t}}{\lVert X_{t}\lVert} < 0$ when the percepton makes no more updates. and the last term $2w_{*}\frac{Y_{t}X_{t}}{\lVert X_{t}\lVert} \geq 1$. Something negative minus something positive is negative, so the whole term is negative. Adding this negative term to the other side we get the inequality:

$$\lVert w_{t} - w_{*} \rVert^{2} \leq \lVert w_{t-1} - w_{*} \rVert^{2} - 1$$

Using this inquality iteratively:

$$\lVert w_{t-1} - w_{*} \rVert^{2} \leq \lVert w_{t-2} - w_{*} \rVert^{2} - 2$$
$$\lVert w_{t-2} - w_{*} \rVert^{2} \leq \lVert w_{t-3} - w_{*} \rVert^{2} - 3$$
$$...$$
$$\lVert w_{*} - w_{*} \rVert^{2} \leq \lVert w_{0} - w_{*} \rVert^{2} - k$$ where $k$ is the number of steps, and the LHS is now 0:

$$k \leq \lVert w_{0} - w_{*} \rVert^{2}$$

The number of steps is less than or equal to the $\lVert w_{0} - w_{*} \rVert^{2}$.


## **Problem 15**

*Part 1*

The expected risk of the data-dependent leave-one-out classifier:

$$\mathbb{E}\bigg\{ R_{n}^{D}(g_{n}) \bigg\} = \mathbb{E}\bigg\{ \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}_{g_{n-1}(X_{i},D_{n,i}) \ne Y_{i}} \bigg\}$$

The RHS is a random variable: it is the sum of the variations on the random data set being used to train the leave-one-out classifier. The law of iterated expectations shows this can be written as:

$$\mathbb{E}\bigg\{ R_{n}^{D}(g_{n}) \bigg\} = \mathbb{E}\bigg\{ \mathbb{E} \{ R(g_{n-1}) | D \} \bigg\} = \mathbb{E} \{ R(g_{n-1}) \}$$

*Part 2*

$$\mathbb{E}\bigg\{ R_{n}^{D}(g_{n}) \bigg\} = \mathbb{E} \{ R(g_{n-1}) \}$$

can be used to bound the expected risk of the perception classifier. The expected risk of the leave-one-out classifier times $n$ is the number of updates made during the perception classifier, (otherwise the $x_{i}$ in the iteration is correctly classified by the algorithm and no update is made). The estimation of the risk based on the leave one out estimator is the same as $M$ the number of iterations made by the preception althorithm. This $M$ is upper-bounded (Novikoff) by $(\frac{R}{\gamma})^{2}$, where R is the distance to the furthest point and gamma is the size of the margin. This is upperbounded by the number of steps bounded in Problem 14.

## **Problem 16**

**Consider the majority classifier:**

$$g_{n}(x, D_{n}) =
    \begin{cases}
      1, & \text{if}\ \sum_{i=1}^{n} Y_{i} \geq \frac{n}{2} \\
      0, & \text{otherwise}
    \end{cases}
$$

**(Thus, $g_{n}$ ignores $x$ and the $X_{i}$â€™s.) Assume that $n$ is odd. What is the expected risk $\mathbb{E}R(g_{n}) = \mathbb{P} \{g_{n}(X) \ne Y\}$ of this classifier? Study the performance of the leave-one-out error estimate. Show that for some distributions $Var(R_{n}^{D}(g_{n})) \geq c/\sqrt{n}$ for some constant $c$. *Hint:* Strang things happen when the number of 0's and 1's is about the same in the data.**

*Solution Part 1:*

Let $N_{n}$ be the number of $Y_{i} = 0$ in the sample. So $N_{n}$ is binomial $(n, 1-p)$ with $p$ the $P\{Y=1\}$.

$$\mathbb{E}(R(g_{n})) = p\mathbb{P}\bigg\{ N_{n} \geq \frac{n}{2} \bigg\} + (1-p)\mathbb{P}\bigg\{ N_{n} < \frac {n}{2} \bigg\} \to min(p, 1-p)$$

*Solution Part 2:*

Minimal variance of the leave-one-out classifier can be estimated when the true probability of either class is $\mathbb{P}\{Y=1\} = \mathbb{P}\{Y=0\} = \frac{1}{2}$ and the probability the data set comes near the true probabilty $\frac{n+1}{2}$ (given $n$ is odd). This probability can be represented as:

$$\mathbb{P}\bigg\{ Bin \Big(n, \frac{1}{2}\Big) = \frac{n+1}{2} \bigg\}$$

This probability is (assuming $n \geq 1$):

$$= 2^{-n} {n \choose \frac{n+1}{2}}$$

Which is always greater than $\frac{c}{\sqrt(n)}$ where c is some positive constant, say $\frac{1}{n+1}$.

