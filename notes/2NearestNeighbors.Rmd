---
title: "Intro to Machine Learning and Binary Classification"
subtitle: "An aggregation of class notes and Lugosi notes"
author: "Aimee Barciauskas"
date: "`r Sys.Date()`"
output: pdf_document
---


# Nearest Neighbor

Suppose that $\mathcal{X}$ is a metric space, distance is a function of $X \in \mathcal{X}$.

Properties of distances:

* $d(x,z) > 0 \to x \ne z$
* $d(x,x) = 0$
* $d(x,z) = d(z,x)$
* $d(x,z) \leq d(x,v) + d(v,z) \text{ the "triangle inquality"}$

Examples of how to measure metrix space: $L_{2} \text{ (Euclidian, proof of the triangle inquality)}, L_{1} \text{ ("Manhattan distance")}, L_{\inf}$

## 1-Nearest Neighbor

Algorithm:

1. Order distances: $d(X_{(1)},x) \leq d(X_{(2)})$
2. Label by nearest point: $g_{n}(x) = Y_{(1)}(x)$

## Proof of proximity

*Why can we assume the nearest neighbor is close?* **EXERCISE**

Answers:

* [Problemset 1, #5](https://app.box.com/files/0/f/4339446734/1/f_51633342485)
* [Nearest Neighbor Notes, page 3](https://app.box.com/files/0/f/4339446734/1/f_49556529122)

## Properties of Nearest Neighbors

* **curse of dimensionality:** As $d$ grows, required points grows exponentially
* The nearest neighbor is *typically* at a distance at most $cn^{-\frac{1}{2}}$ where $c$ is a constant. Here *typically* indicates that $X$ has a positive and smooth density.

**Theorem:** the risk of the NN classifier is such that, for all distributions of $(X,Y)$:

$$R(g_{n}) \to 2\mathbb{E}[\eta(x)(1-\eta(x))]$$

and

$$R^{*} \leq R^{NN} \leq 2R^{*}$$

In particular if $R^{*} = 0$ then $R^{NN} = 0$

## K-Nearest Neighbor

Let $k \geq 1$ be an odd integer.

Let

$$g_{n}(x) =
\begin{cases}
1 \text{ if } \sum Y_{i}(x) > \frac{k}{2}\\
0 \text{ otherwise}\\
\end{cases}$$

Why bother?

Suppose $Y \sim Bern(0.9)$ and $Y' \sim Bern(0.9)$. Then, 1-NN risk is $2p(1-p)$.

If $Y' = (Y_{1}',...,Y_{k}')$, risk is $\mathbb{E}\{\mathbb{P}[majority((Y_{1}',...,Y_{k}')) \ne Y|X]\}$

For example, for 3-NN:

$$\mathbb{P}(majority((Y_{1}',Y_{2}',Y_{3}')) \ne Y) = \eta(1-\eta)^{3} + 3\eta^{2}(1-\eta)^{2} + \eta^{3}(1-\eta) + 3\eta^{2}(1-\eta)^{2}$$

e.g. first term is probability all of $Y' \ne Y$, second term equivalent to probability 2 of $Y' \ne Y$

This can be reduced to: $R^{3-NN} = \mathbb{E}[\eta(x)(1-\eta(x))] + 4\mathbb{E}[\eta(x)^{2}(1-\eta(x))^{2}]$ which "is peanuts"

$R^{k-NN}$ in the general case: **EXERCISE**

[Nearest Neighbor Notes, page 8](https://app.box.com/files/0/f/4339446734/1/f_49556529122)

### Corollaries

$$2R^{*} \geq R^{k-NN} \geq R^{*}$$

This is all distribution free! :)
But it is also asymptotic :(

## K as a function of n

One can take $k = k(n)$

Still we have

$$\lVert X_{k(n)} - x \rVert \to 0 \text{ as $n\to\infty$ }$$

if $\frac{k(n)}{n} \to 0$

Moreover, if $k(n) \to \infty$ and $\frac{k(n)}{n} \to 0$, then, for all distributions:

$$R(g_{n}^{k(n)}) \to R^{*} \text{ in probability}$$


* (LLN?) $k(n) \to \infty$ is important for averaging to work, the variance is small.
* $\frac{k(n)}{n} \to 0$ implies bias is small

**Thus, the $k(n)-NN$ rule is univisally consistent**

> The real challenge in machine learning is to focus on where the action is. (Gabor)

## Other local averaging rules

### 1. Histogram Rules

Split the feature space into "small" cells and take the majority vote of points in the same cell. In $\mathbb{R}^{d}$ one may partitiaon the space into cubes of length $h$. Same tradeoffs fro $h$ as $k$ in $k-nn$
    * **One can prove that if $h(n) \to 0$ and $n(h(n)^{d}) \to \infty$ where $n(h(n)^{d})$ is the volume of the hyper cell.

**Curse of Dimensionality**

Suppose $X$ is uniformly distributed $[0,1]^{d}$, choose $h$ such that each box contains 1% of the data on average.

$$0.01 = volume(cube) = h^{d} \text{ entire space has volume 1}$$

\begin{table}[]
\centering
\caption{}
\label{my-label}
\begin{tabular}{lllll}
d (dimension) & h (length of one side of hypercube) &  &  &  \\
1             & 0.01                                &  &  &  \\
2             & 0.1                                 &  &  &  \\
10            & 0.63 (how is this local?)           &  &  &  \\
100           & 0.955                               &  &  &  \\
1000          & 0.9954                              &  &  & 
\end{tabular}
\end{table}

### 2. Kernel / Nataraya-Wabon / Parzen Classifiers

Given $K : \mathbb{R}^{d} \to \mathbb{R} \text{ (typically positive and decreasing away from 0)}$

$$g_{n}(x) =
\left\{
	\begin{array}{ll}
		1  & \mbox{if } \sum_{i: Y_{i} = 1} K \frac{x-X_{i}}{h(n)} >  \sum_{i: Y_{i} = 0} K \frac{x-X_{i}}{h(n)}\\
		0 \mbox{ otherwise}
	\end{array}
\right.$$

The first term acts as a weight, while the second term makes it local.
