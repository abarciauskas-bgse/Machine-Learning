---
title: Machine Learning Topic 3
subtitle: Error Estimation
author: Aimee Barciauskas
date: 29 March 2016
output: pdf_document
---

# Error Estimation

* Definition of **empirical error**: *slide 1*
    * This may be misleading when $g = g_{n}$ is a data-based classifier, e.g. a data-based classifier may demonstrate 0 empirical error which is not representative of the true error of the classifier.
* LLN for $R_{n}(g)$: *slide 1*
* CLT for empirical deviation from true risk: *slide 1*
* Proof of Chebyshev: *slide 2*
* Typical deviations from expected values: *slide 3*
* Chernoff Bounds: *slide 4*
* Prime example for Chernoff Bounds: *slide 5*
* Hoeffding's Lemma: *slide 5*
* Hoeffding's Inequality: *slide 5*
* Bernstein's Inequality: *slide 6*
* Bound on the true risk of data-based classifier from the empirical risk: *slide 7*
* Bound on the true risk of data-based classifier from the best in class: *slide 7*
* Proof the empirical risk minimizier is PAC: *bottom of slide 7-8*
* Bound on the true risk of data-based classifier when empirical risk is 0 and best in class is 0: *slide 9*
* Other error estimates: *slide 10*
* Leave-one-out, k-cross validation: *slide 11*
* Posterior Probability Estimates: *slide 12*

