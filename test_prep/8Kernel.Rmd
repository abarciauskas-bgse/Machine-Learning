---
title: Machine Learning Topic 8
subtitle: Kernel Methods
author: Aimee Barciauskas
date: 29 March 2016
output: pdf_document
---



The moving window rule:

$$
g_{n}(x) =
\left\{
  \begin{array}{ll}
    0  & \mbox{if } \sum_{i=1}^{n}\mathbb{I}_{Y_{i}=0, X_{i} \in S_{x,h}} \geq \sum_{i=1}^{n}\mathbb{I}_{Y_{i}=1, X_{i} \in S_{x,h}}\\
    1 & \mbox{otherwise }
  \end{array}
\right.
$$


The kernel-classification rule:

$$
g_{n}(x) =
\left\{
  \begin{array}{ll}
    0  & \mbox{if } \sum_{i=1}^{n}\mathbb{I}_{Y_{i}=0}K(\frac{x-X_{i}}{h}) \geq \sum_{i=1}^{n}\mathbb{I}_{Y_{i}=1}K(\frac{x-X_{i}}{h})\\
    1 & \mbox{otherwise }
  \end{array}
\right.
$$

Clearly, the kernel rule is a generalization of the moving window rule, since taking the special kernel $K(x) = \mathbb{I}_{x \in S_{0,1}}$ yields the moving window rule.


We state the universal consistency theorem for a large class of kernel functions, namely, for all regular kernels:

**Definition 10.1.** The kernel K is called regular if it is nonnegative, and there is a ball $S_{0}$, $r$ of radius $r > 0$ centered at the origin, and constant $b > 0$ such that $K(x) \geq b \mathbb{I}_{S_{0},r}$ and $\int sup_{y \in x + S_{0}} K(y) dx < \infty$

## Theorem 10.1

Assume that K is a regular kernel. If $h \to 0$ and $nh^{d} \to \infty$ as $n \to \infty$,

then for any distribution of $(X, Y )$, and for every $\epsilon > 0$ there is an integer $n_{0}$ such that for $n > n_{0}$ for the error probability $L_{n}$ of the kernel rule:


$P\{L_{n} - L^{*} > \epsilon\} \leq 2e^{-n\epsilon^{2}/32\rho^{2}}$


where the constant $\rho$ depends on the kernel K and the dimension only. Thus, the kernel rule is strongly universally consistent.

## Trivial example

Take $n = 1$ and $h = 1$, we have the classifier:


$$
g_{1}(x) =
\left\{
  \begin{array}{ll}
    0  & \mbox{if } Y = 0, |x-X_{1}| \mbox{ or if } Y = 1, |x-X_{1}| \geq 1\\
    1 & \mbox{otherwise }
  \end{array}
\right.
$$

If $K > 0$ everywhere:

$\mathbb{E}L_{1} = \mathbb{P}\{Y_{1}=0,Y=1\} + \mathbb{P}\{Y_{1}=1,Y=0\} = 2\mathbb{E}[\eta(x)]\mathbb{E}[1-\eta(x)]$

which may be $\frac{1}{2}$ (if the expected value of $\eta(x)$ is $\frac{1}{2}$) even if $L^{*}$ is 0 (which happens when $\eta \in \{0,1\}$ everywhere). If $K \equiv 1$, we ignore the $X_{i}$'s and take a majority vote:

$$
g_{n}(x) =
\left\{
  \begin{array}{ll}
    0  & \mbox{if } \sum_{i=1}^{n} \mathbb{I}_{Y_{i}=0} \geq \sum_{i=1}^{n} \mathbb{I}_{Y_{i}=1}\\
    1 & \mbox{otherwise }
  \end{array}
\right.
$$

Let $N_{n}$ be the number of $Y_{i}$'s equal to zero. As $N_{n}$ is binomial $(n, 1-p)$ with $p=\mathbb{E}\eta(X)=\mathbb{E}\{Y=1\}$, we see that:

$$\mathbb{E}L_{n} = p\mathbb{P}\bigg\{N_{n} \geq \frac{n}{2} \bigg\} + (1-p)\mathbb{P}\bigg\{N_{n} \leq \frac{n}{2}\bigg\} \to min(p, 1-p)$$

It is interesting to note the following:

$\mathbb{E} = 2p(1-p)$

$= 2min(p(1-p))(1-min(p,1-p))$

$\leq 2p(1-p)$

$= 2\lim\limits_{n \to \infty} \mathbb{E}L_{n}$

The expected error with one observation is at most twice as bad as the expected error with an infinite sequence. 






